{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transformer 기반 ASR전체코드"
      ],
      "metadata": {
        "id": "TjpyQ-DKDbKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQkts5u1DZa3",
        "outputId": "13b3c89e-e68f-4ba0-806b-d69eb94ed1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 Loss: 1.4340\n",
            "Epoch 01 Loss: 1.4340\n",
            "Epoch 02 Loss: 1.2252\n",
            "Epoch 02 Loss: 1.2252\n",
            "Epoch 03 Loss: 1.1517\n",
            "Epoch 03 Loss: 1.1517\n",
            "Epoch 04 Loss: 1.0814\n",
            "Epoch 04 Loss: 1.0814\n",
            "Epoch 05 Loss: 1.0034\n",
            "Epoch 05 Loss: 1.0034\n",
            "Epoch 06 Loss: 0.9134\n",
            "Epoch 06 Loss: 0.9134\n",
            "Epoch 07 Loss: 0.8193\n",
            "Epoch 07 Loss: 0.8193\n",
            "Epoch 08 Loss: 0.7535\n",
            "Epoch 08 Loss: 0.7535\n",
            "Epoch 09 Loss: 0.6884\n",
            "Epoch 09 Loss: 0.6884\n",
            "Epoch 10 Loss: 0.6414\n",
            "Epoch 10 Loss: 0.6414\n",
            "Epoch 11 Loss: 0.6073\n",
            "Epoch 11 Loss: 0.6073\n",
            "Epoch 12 Loss: 0.5734\n",
            "Epoch 12 Loss: 0.5734\n",
            "Epoch 13 Loss: 0.5445\n",
            "Epoch 13 Loss: 0.5445\n",
            "Epoch 14 Loss: 0.5328\n",
            "Epoch 14 Loss: 0.5328\n",
            "Epoch 15 Loss: 0.5111\n",
            "Epoch 15 Loss: 0.5111\n",
            "Epoch 16 Loss: 0.4834\n",
            "Epoch 16 Loss: 0.4834\n",
            "Epoch 17 Loss: 0.4658\n",
            "Epoch 17 Loss: 0.4658\n",
            "Epoch 18 Loss: 0.4506\n",
            "Epoch 18 Loss: 0.4506\n",
            "Epoch 19 Loss: 0.4368\n",
            "Epoch 19 Loss: 0.4368\n",
            "Epoch 20 Loss: 0.4241\n",
            "Epoch 20 Loss: 0.4241\n",
            "Epoch 21 Loss: 0.4252\n",
            "Epoch 21 Loss: 0.4252\n",
            "Epoch 22 Loss: 0.4246\n",
            "Epoch 22 Loss: 0.4246\n",
            "Epoch 23 Loss: 0.4000\n",
            "Epoch 23 Loss: 0.4000\n",
            "Epoch 24 Loss: 0.4122\n",
            "Epoch 24 Loss: 0.4122\n",
            "Epoch 25 Loss: 0.3940\n",
            "Epoch 25 Loss: 0.3940\n",
            "Epoch 26 Loss: 0.3589\n",
            "Epoch 26 Loss: 0.3589\n",
            "Epoch 27 Loss: 0.3661\n",
            "Epoch 27 Loss: 0.3661\n",
            "Epoch 28 Loss: 0.3480\n",
            "Epoch 28 Loss: 0.3480\n",
            "Epoch 29 Loss: 0.3457\n",
            "Epoch 29 Loss: 0.3457\n",
            "Epoch 30 Loss: 0.3388\n",
            "Epoch 30 Loss: 0.3388\n",
            "라벨 : 저 식당 음식이 정말 맛있나 봐요.\n",
            "라벨 : 저 식당 음식이 정말 맛있나 봐요.\n",
            "추론 결과: <unk><unk> <unk><unk><unk><unk><unk> <unk>깝<unk>가 <unk><unk>고 <unk>각<unk><unk>.<unk><unk><unk><unk><unk> <unk><unk><unk> <unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk><unk> <unk><unk><unk>까<unk> <unk><unk><unk><unk>고 <unk><unk><unk><unk><unk>고 <unk><unk><unk><unk><unk>고 그<unk><unk> <unk><unk><unk>같<unk>.\n",
            "추론 결과: <unk><unk> <unk><unk><unk><unk><unk> <unk>깝<unk>가 <unk><unk>고 <unk>각<unk><unk>.<unk><unk><unk><unk><unk> <unk><unk><unk> <unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk><unk> <unk><unk><unk>까<unk> <unk><unk><unk><unk>고 <unk><unk><unk><unk><unk>고 <unk><unk><unk><unk><unk>고 그<unk><unk> <unk><unk><unk>같<unk>.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import math\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "ROOT_DIR = os.getcwd()\n",
        "\n",
        "# 1) 데이터 다운로드 및 압축 해제\n",
        "TAR_PATH = os.path.join(ROOT_DIR, \"KoreanReadSpeechCorpus.tar.gz\")\n",
        "if not os.path.isdir(os.path.join(ROOT_DIR, \"KoreanReadSpeechCorpus\")):\n",
        "    subprocess.run(\n",
        "        [\"wget\", \"-O\", TAR_PATH, \"https://www.openslr.org/resources/97/KoreanReadSpeechCorpus.tar.gz\"],\n",
        "        check=True\n",
        "    )\n",
        "    subprocess.run([\"tar\", \"-xvzf\", TAR_PATH], check=True)\n",
        "\n",
        "# 2) Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000): #초기화 method\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)#0으로 초기화\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) #position 정보\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) #sin과 cos으로 position을 잡는다.\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "# 3) Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None): #순전파 학습 알고리즘\n",
        "        B = q.size(0)\n",
        "        Q = self.w_q(q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.w_k(k).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.w_v(v).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        if mask is not None:\n",
        "            ext_mask = mask.unsqueeze(0).unsqueeze(0)\n",
        "            scores = scores.masked_fill(ext_mask == 0, -1e9)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        ctx = torch.matmul(attn, V)\n",
        "        ctx = ctx.transpose(1, 2).contiguous().view(B, -1, self.num_heads * self.d_k)\n",
        "        return self.w_o(ctx)\n",
        "\n",
        "# 4) Feed-Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "# 5) Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        a = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.drop(a))\n",
        "        f = self.ff(x)\n",
        "        x = self.norm2(x + self.drop(f))\n",
        "        return x\n",
        "\n",
        "# 6) Transformer Decoder Layer\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, memory, tgt_mask=None, mem_mask=None):\n",
        "        a1 = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.drop(a1))\n",
        "        a2 = self.cross_attn(x, memory, memory, mem_mask)\n",
        "        x = self.norm2(x + self.drop(a2))\n",
        "        f = self.ff(x)\n",
        "        x = self.norm3(x + self.drop(f))\n",
        "        return x\n",
        "\n",
        "# 7) Audio Feature Extractor\n",
        "class AudioFeatureExtractor(nn.Module):\n",
        "    def __init__(self, d_model=512):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(80, 256, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(512, d_model, kernel_size=3, padding=1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "# 8) Korean ASR Transformer Model\n",
        "class KoreanASRTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, num_heads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6,\n",
        "                 d_ff=2048, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.audio_extractor = AudioFeatureExtractor(d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
        "        self.tgt_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "        return mask == 0\n",
        "\n",
        "    def forward(self, audio: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        mem = self.audio_extractor(audio)\n",
        "        mem = self.pos_enc(mem * math.sqrt(mem.size(-1)))\n",
        "        mem = self.dropout(mem)\n",
        "        for layer in self.encoder_layers:\n",
        "            mem = layer(mem)\n",
        "\n",
        "        tgt = self.tgt_emb(target) * math.sqrt(self.tgt_emb.embedding_dim)\n",
        "        tgt = self.pos_enc(tgt)\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "        out = tgt\n",
        "        for layer in self.decoder_layers:\n",
        "            out = layer(out, mem, tgt_mask, None)\n",
        "\n",
        "        return self.out_proj(out)\n",
        "\n",
        "# 9) Dataset for Korean Read Speech Corpus\n",
        "class KoreanSpeechDataset(Dataset):\n",
        "    def __init__(self, json_path: str, root_dir: str):\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = self._build_tokenizer()\n",
        "        self.samples = self._load_metadata(json_path)\n",
        "\n",
        "    def _build_tokenizer(self):\n",
        "        chars = ['ㄱ','ㄴ','ㄷ','ㄹ','ㅁ','ㅂ','ㅅ','ㅇ','ㅈ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ',\n",
        "                 'ㅏ','ㅑ','ㅓ','ㅕ','ㅗ','ㅛ','ㅜ','ㅠ','ㅡ','ㅣ']\n",
        "        syllables = [chr(i) for i in range(0xAC00, 0xD7A4)]\n",
        "        english = [*map(chr, range(ord('a'), ord('z')+1)), *map(chr, range(ord('A'), ord('Z')+1))]\n",
        "        digits = [str(i) for i in range(10)]\n",
        "        specials = [' ','.',',','!','?','-',\"'\" ]\n",
        "        vocab = ['<pad>','<sos>','<eos>','<unk>'] + chars + syllables[:1000] + english + digits + specials\n",
        "        c2i = {c:i for i,c in enumerate(vocab)}\n",
        "        i2c = {i:c for c,i in c2i.items()}\n",
        "        return {'char_to_idx': c2i, 'idx_to_char': i2c, 'vocab_size': len(vocab)}\n",
        "\n",
        "    def _load_metadata(self, json_path: str):\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            meta = json.load(f)\n",
        "        samples = []\n",
        "        for loc, utts in meta.items():\n",
        "            for uid, info in utts.items():\n",
        "                path = os.path.join(self.root_dir, loc, f\"{uid}.wav\")\n",
        "                samples.append({'audio_path': path, 'text': info.get('text','')})\n",
        "        return samples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        s = self.samples[idx]\n",
        "        wav, sr = torchaudio.load(s['audio_path'])\n",
        "        if sr != 16000:\n",
        "            wav = torchaudio.transforms.Resample(sr,16000)(wav)\n",
        "        mel = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=16000, n_mels=80, n_fft=1024, hop_length=256, win_length=1024\n",
        "        )(wav)\n",
        "        mel = torch.log(mel + 1e-9).squeeze(0)\n",
        "        seq = [self.tokenizer['char_to_idx']['<sos>']]\n",
        "        for ch in s['text']:\n",
        "            seq.append(self.tokenizer['char_to_idx'].get(ch, self.tokenizer['char_to_idx']['<unk>']))\n",
        "        seq.append(self.tokenizer['char_to_idx']['<eos>'])\n",
        "        return {'audio_features': mel, 'text_sequence': torch.tensor(seq, dtype=torch.long)}\n",
        "\n",
        "# 10) Collate function\n",
        "def collate_fn(batch):\n",
        "    audios = [b['audio_features'] for b in batch]\n",
        "    texts = [b['text_sequence'] for b in batch]\n",
        "    max_t = max(a.shape[1] for a in audios)\n",
        "    max_l = max(t.size(0) for t in texts)\n",
        "    pa = torch.zeros(len(batch), 80, max_t)\n",
        "    pt = torch.zeros(len(batch), max_l, dtype=torch.long)\n",
        "    for i, (a, t) in enumerate(zip(audios, texts)):\n",
        "        pa[i, :, :a.shape[1]] = a\n",
        "        pt[i, :t.size(0)] = t\n",
        "    return {'audio_features': pa, 'text_sequences': pt}\n",
        "\n",
        "# 11) Training loop\n",
        "def train(model, dataloader, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    model.to(device).train()\n",
        "    for epoch in range(1, 31):\n",
        "        total_loss = 0.0\n",
        "        for batch in dataloader:\n",
        "            audio = batch['audio_features'].to(device)\n",
        "            text = batch['text_sequences'].to(device)\n",
        "            inp, tgt = text[:, :-1], text[:, 1:]\n",
        "            logits = model(audio, inp)\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt.reshape(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch:02d} Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# 12) Greedy Inference\n",
        "def infer(model, dataset, audio_path, device, max_len=100):\n",
        "    model.to(device).eval()\n",
        "    wav, sr = torchaudio.load(audio_path)\n",
        "    if sr != 16000:\n",
        "        wav = torchaudio.transforms.Resample(sr,16000)(wav)\n",
        "    mel = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=16000, n_mels=80, n_fft=1024, hop_length=256, win_length=1024\n",
        "    )(wav)\n",
        "    mel = torch.log(mel + 1e-9).squeeze(0).unsqueeze(0).to(device)\n",
        "    seq = torch.tensor([[dataset.tokenizer['char_to_idx']['<sos>']]], dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            logits = model(mel, seq)\n",
        "            nxt = logits[0, -1].argmax().unsqueeze(0).unsqueeze(0)\n",
        "            if nxt.item() == dataset.tokenizer['char_to_idx']['<eos>']:\n",
        "                break\n",
        "            seq = torch.cat([seq, nxt], dim=1)\n",
        "    res = \"\"\n",
        "    for idx in seq[0, 1:]:\n",
        "        ch = dataset.tokenizer['idx_to_char'].get(idx.item(), \"\")\n",
        "        if ch == \"<eos>\":\n",
        "            break\n",
        "        res += ch\n",
        "    return res\n",
        "\n",
        "# 13) Main\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    JSON_PATH = os.path.join(ROOT_DIR, \"Korean_Read_Speech_Corpus_sample.json\")\n",
        "    ds = KoreanSpeechDataset(JSON_PATH, ROOT_DIR)\n",
        "    loader = DataLoader(ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "    model = KoreanASRTransformer(vocab_size=ds.tokenizer['vocab_size'])\n",
        "    train(model, loader, device)\n",
        "    torch.save(model.state_dict(), \"korspeech_transformer.pth\")\n",
        "    sample = ds.samples[0]\n",
        "    print(\"라벨 :\", sample['text'])\n",
        "    print(\"추론 결과:\", infer(model, ds, sample['audio_path'], device))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "사전 학습 모델"
      ],
      "metadata": {
        "id": "4ff2ugubYhQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch librosa ipywidgets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yf0FcPlYi5n",
        "outputId": "9534ad6c-30a4-4c02-9542-2e919b41b0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed jedi-0.19.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from IPython.display import Audio, display, HTML, Javascript\n",
        "import ipywidgets as widgets\n",
        "from google.colab import output\n",
        "import io\n",
        "import base64\n",
        "import wave\n",
        "\n",
        "# 전역 변수로 모델 초기화\n",
        "model = None\n",
        "processor = None\n",
        "\n",
        "def init_model():\n",
        "    \"\"\"모델 초기화 (한 번만 실행)\"\"\"\n",
        "    global model, processor\n",
        "\n",
        "    if model is None:\n",
        "        print(\"Wav2Vec2 한국어 모델 로딩 중...\")\n",
        "\n",
        "        # 추천 pre-trained 모델 목록 (성능순)\n",
        "        # 1. w11wo/wav2vec2-xls-r-300m-korean (가장 큰 모델, 높은 성능)\n",
        "        # 2. kresnik/wav2vec2-large-xlsr-korean (현재 사용 중)\n",
        "        # 3. Hyuk/wav2vec2-korean-v2 (v2 버전)\n",
        "        # 4. hyyoka/wav2vec2-xlsr-korean-senior (시니어 특화)\n",
        "\n",
        "        model_name = \"kresnik/wav2vec2-large-xlsr-korean\"  # 더 큰 모델로 변경\n",
        "\n",
        "        try:\n",
        "            processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            print(f\"모델 로딩 완료! 사용 모델: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"메인 모델 로딩 실패: {e}\")\n",
        "            print(\"백업 모델로 전환...\")\n",
        "            # 백업 모델\n",
        "            model_name = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
        "            processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            print(f\"백업 모델 로딩 완료: {model_name}\")\n",
        "\n",
        "def init_model_with_choice(model_choice=\"best\"):\n",
        "    \"\"\"\n",
        "    모델 선택 옵션을 제공하는 초기화 함수\n",
        "\n",
        "    Args:\n",
        "        model_choice: 모델 선택\n",
        "            - \"best\": 가장 성능이 좋은 모델 (기본값)\n",
        "            - \"fast\": 빠른 처리 속도\n",
        "            - \"balanced\": 균형잡힌 성능\n",
        "            - \"senior\": 시니어 음성 특화\n",
        "    \"\"\"\n",
        "    global model, processor\n",
        "\n",
        "    if model is None:\n",
        "        print(\"Wav2Vec2 한국어 모델 로딩 중...\")\n",
        "\n",
        "        # 모델 선택 매핑\n",
        "        model_options = {\n",
        "            \"best\": \"w11wo/wav2vec2-xls-r-300m-korean\",        # 가장 큰 모델, 높은 성능\n",
        "            \"fast\": \"Kkonjeong/wav2vec2-base-korean\",          # 빠른 처리\n",
        "            \"balanced\": \"kresnik/wav2vec2-large-xlsr-korean\",  # 균형 잡힌 성능\n",
        "            \"senior\": \"hyyoka/wav2vec2-xlsr-korean-senior\"     # 시니어 특화\n",
        "        }\n",
        "\n",
        "        model_name = model_options.get(model_choice, model_options[\"best\"])\n",
        "\n",
        "        try:\n",
        "            processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            print(f\"모델 로딩 완료! 사용 모델: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"선택한 모델 로딩 실패: {e}\")\n",
        "            print(\"기본 모델로 전환...\")\n",
        "            # 기본 모델\n",
        "            model_name = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
        "            processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            print(f\"기본 모델 로딩 완료: {model_name}\")\n",
        "\n",
        "def wav2vec2_korean_stt(audio_data, sample_rate=16000):\n",
        "    \"\"\"\n",
        "    Wav2Vec2 한국어 모델로 음성인식\n",
        "\n",
        "    Args:\n",
        "        audio_data: 오디오 데이터 (numpy array)\n",
        "        sample_rate: 샘플레이트\n",
        "\n",
        "    Returns:\n",
        "        인식된 텍스트\n",
        "    \"\"\"\n",
        "    global model, processor\n",
        "\n",
        "    # 모델이 로드되지 않았으면 초기화\n",
        "    if model is None:\n",
        "        init_model()\n",
        "\n",
        "    # 오디오 데이터 전처리\n",
        "    if len(audio_data) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # 16kHz로 리샘플링 (필요한 경우)\n",
        "    if sample_rate != 16000:\n",
        "        audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)\n",
        "\n",
        "    # 전처리\n",
        "    inputs = processor(\n",
        "        audio_data,\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # 예측 수행\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs.input_values).logits\n",
        "\n",
        "    # 가장 확률이 높은 토큰 선택\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # 텍스트로 디코딩\n",
        "    text = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "    return text\n",
        "\n",
        "def setup_microphone_recording():\n",
        "\n",
        "    # JavaScript 코드로 마이크 녹음 구현\n",
        "    js_code = \"\"\"\n",
        "    <script>\n",
        "    let mediaRecorder;\n",
        "    let audioChunks = [];\n",
        "    let isRecording = false;\n",
        "\n",
        "    async function startRecording() {\n",
        "        try {\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "            mediaRecorder = new MediaRecorder(stream);\n",
        "            audioChunks = [];\n",
        "\n",
        "            mediaRecorder.ondataavailable = event => {\n",
        "                audioChunks.push(event.data);\n",
        "            };\n",
        "\n",
        "            mediaRecorder.onstop = async () => {\n",
        "                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
        "                const arrayBuffer = await audioBlob.arrayBuffer();\n",
        "                const base64Audio = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));\n",
        "\n",
        "                // Python으로 오디오 데이터 전송\n",
        "                google.colab.kernel.invokeFunction('process_audio', [base64Audio], {});\n",
        "            };\n",
        "\n",
        "            mediaRecorder.start();\n",
        "            isRecording = true;\n",
        "\n",
        "            document.getElementById('recordBtn').innerText = '녹음 중... (클릭하여 중지)';\n",
        "            document.getElementById('recordBtn').style.backgroundColor = '#ff4444';\n",
        "\n",
        "        } catch (err) {\n",
        "            console.error('마이크 접근 오류:', err);\n",
        "            alert('마이크 접근 권한이 필요합니다.');\n",
        "        }\n",
        "    }\n",
        "\n",
        "    function stopRecording() {\n",
        "        if (mediaRecorder && isRecording) {\n",
        "            mediaRecorder.stop();\n",
        "            isRecording = false;\n",
        "\n",
        "            document.getElementById('recordBtn').innerText = '음성 인식 중...';\n",
        "            document.getElementById('recordBtn').disabled = true;\n",
        "\n",
        "            // 스트림 중지\n",
        "            mediaRecorder.stream.getTracks().forEach(track => track.stop());\n",
        "        }\n",
        "    }\n",
        "\n",
        "    function toggleRecording() {\n",
        "        if (isRecording) {\n",
        "            stopRecording();\n",
        "        } else {\n",
        "            startRecording();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    function resetButton() {\n",
        "        document.getElementById('recordBtn').innerText = ' 녹음 시작';\n",
        "        document.getElementById('recordBtn').style.backgroundColor = '#4CAF50';\n",
        "        document.getElementById('recordBtn').disabled = false;\n",
        "    }\n",
        "    </script>\n",
        "\n",
        "    <div style=\"text-align: center; margin: 20px;\">\n",
        "        <button id=\"recordBtn\" onclick=\"toggleRecording()\"\n",
        "                style=\"padding: 15px 30px; font-size: 16px; background-color: #4CAF50;\n",
        "                       color: white; border: none; border-radius: 5px; cursor: pointer;\">\n",
        "             녹음 시작\n",
        "        </button>\n",
        "        <div id=\"result\" style=\"margin-top: 20px; padding: 10px;\n",
        "                              background-color: #f0f0f0; border-radius: 5px;\n",
        "                              min-height: 50px; font-size: 14px;\">\n",
        "            인식 결과가 여기에 표시됩니다...\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    display(HTML(js_code))\n",
        "\n",
        "def process_audio(audio_base64):\n",
        "    \"\"\"녹음된 오디오 처리\"\"\"\n",
        "    try:\n",
        "        # Base64 디코딩\n",
        "        audio_data = base64.b64decode(audio_base64)\n",
        "\n",
        "        # 임시 파일로 저장\n",
        "        import tempfile\n",
        "        import os\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:\n",
        "            temp_file.write(audio_data)\n",
        "            temp_filename = temp_file.name\n",
        "\n",
        "        try:\n",
        "            # librosa로 오디오 로드 (webm 형식 처리)\n",
        "            audio_array, sr = librosa.load(temp_filename, sr=16000)\n",
        "\n",
        "            # 음성인식 수행\n",
        "            if len(audio_array) > 0:\n",
        "                result_text = wav2vec2_korean_stt(audio_array)\n",
        "\n",
        "                # 결과 표시\n",
        "                display(HTML(f\"\"\"\n",
        "                <script>\n",
        "                    document.getElementById('result').innerHTML = '<strong>인식 결과:</strong> {result_text}';\n",
        "                    resetButton();\n",
        "                </script>\n",
        "                \"\"\"))\n",
        "\n",
        "                print(f\"인식 결과: {result_text}\")\n",
        "            else:\n",
        "                display(HTML(\"\"\"\n",
        "                <script>\n",
        "                    document.getElementById('result').innerHTML = '<strong>오류:</strong> 음성이 감지되지 않았습니다.';\n",
        "                    resetButton();\n",
        "                </script>\n",
        "                \"\"\"))\n",
        "\n",
        "        finally:\n",
        "            # 임시 파일 삭제\n",
        "            if os.path.exists(temp_filename):\n",
        "                os.unlink(temp_filename)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = str(e).replace(\"'\", \"\\\\'\")\n",
        "        display(HTML(f\"\"\"\n",
        "        <script>\n",
        "            document.getElementById('result').innerHTML = '<strong>오류:</strong> {error_msg}';\n",
        "            resetButton();\n",
        "        </script>\n",
        "        \"\"\"))\n",
        "        print(f\"오류 발생: {e}\")\n",
        "\n",
        "def start_realtime_stt(model_choice=\"best\"):\n",
        "    \"\"\"\n",
        "    실시간 STT 시작\n",
        "\n",
        "    Args:\n",
        "        model_choice: 모델 선택\n",
        "            - \"best\": w11wo/wav2vec2-xls-r-300m-korean (가장 높은 성능)\n",
        "            - \"fast\": Kkonjeong/wav2vec2-base-korean (빠른 처리)\n",
        "            - \"balanced\": kresnik/wav2vec2-large-xlsr-korean (균형)\n",
        "            - \"senior\": hyyoka/wav2vec2-xlsr-korean-senior (시니어 특화)\n",
        "    \"\"\"\n",
        "    print(\"=== 실시간 한국어 음성인식 ===\")\n",
        "    print(\"1. 모델 초기화 중...\")\n",
        "\n",
        "    # 모델 초기화\n",
        "    init_model_with_choice(model_choice)\n",
        "\n",
        "    print(\"2. 마이크 녹음 인터페이스 설정...\")\n",
        "\n",
        "    # 오디오 처리 함수 등록\n",
        "    output.register_callback('process_audio', process_audio)\n",
        "\n",
        "    # 마이크 녹음 UI 설정\n",
        "    setup_microphone_recording()\n",
        "\n",
        "    print(\"3. 설정 완료!\")\n",
        "    print(\"위의 '🎤 녹음 시작' 버튼을 클릭하여 음성인식을 시작하세요.\")\n",
        "    print(\"녹음을 중지하려면 버튼을 다시 클릭하세요.\")\n",
        "\n",
        "    # 모델 정보 출력\n",
        "    print(f\"\\n 모델 성능 정보:\")\n",
        "    model_info = {\n",
        "        \"best\": \"w11wo/wav2vec2-xls-r-300m-korean - 가장 높은 정확도 (300M 파라미터)\",\n",
        "        \"fast\": \"Kkonjeong/wav2vec2-base-korean - 빠른 처리 속도\",\n",
        "        \"balanced\": \"kresnik/wav2vec2-large-xlsr-korean - 균형잡힌 성능\",\n",
        "        \"senior\": \"hyyoka/wav2vec2-xlsr-korean-senior - 시니어 음성 특화\"\n",
        "    }\n",
        "    print(f\"현재 사용 모델: {model_info.get(model_choice, model_info['best'])}\")\n",
        "\n",
        "    print(\"\\n💡 다른 모델 사용법:\")\n",
        "    print(\"start_realtime_stt('fast')      # 빠른 처리\")\n",
        "    print(\"start_realtime_stt('balanced')  # 균형잡힌 성능\")\n",
        "    print(\"start_realtime_stt('senior')    # 시니어 특화\")\n",
        "\n",
        "def simple_file_stt(audio_file):\n",
        "    \"\"\"파일 기반 STT (기존 기능 유지)\"\"\"\n",
        "    init_model()\n",
        "\n",
        "    # 오디오 로드\n",
        "    audio, sr = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "    # 음성인식\n",
        "    result = wav2vec2_korean_stt(audio)\n",
        "\n",
        "    return result\n",
        "\n",
        "# 사용 예시\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 필요한 라이브러리 설치 안내\n",
        "    print(\"필요한 라이브러리 설치:\")\n",
        "    print(\"!pip install transformers torch librosa ipywidgets\")\n",
        "    print(\"!apt-get update && apt-get install -y ffmpeg\")  # ffmpeg 추가\n",
        "    print()\n",
        "\n",
        "    # 실시간 STT 시작 (최고 성능 모델 사용)\n",
        "    start_realtime_stt(\"best\")\n",
        "\n",
        "    print(\"\\n 추천 모델 성능 순위:\")\n",
        "    print(\"1. w11wo/wav2vec2-xls-r-300m-korean (가장 높은 정확도)\")\n",
        "    print(\"2. kresnik/wav2vec2-large-xlsr-korean (현재 기본 모델)\")\n",
        "    print(\"3. Hyuk/wav2vec2-korean-v2 (개선된 v2 버전)\")\n",
        "    print(\"4. hyyoka/wav2vec2-xlsr-korean-senior (시니어 음성 특화)\")\n",
        "\n",
        "    print(\"\\n💡 사용 예시:\")\n",
        "    print(\"start_realtime_stt('best')      # 최고 성능 모델\")\n",
        "    print(\"start_realtime_stt('fast')      # 빠른 처리 모델\")\n",
        "    print(\"start_realtime_stt('senior')    # 시니어 특화 모델\")\n",
        "\n",
        "    # 파일 기반 STT 예시 (옵션)\n",
        "    # result = simple_file_stt(\"audio_file.wav\")\n",
        "    # print(f\"파일 인식 결과: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "eON4nDPNYjMw",
        "outputId": "9304dc4f-d039-4a4f-8b78-12b6524104c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "필요한 라이브러리 설치:\n",
            "!pip install transformers torch librosa ipywidgets\n",
            "!apt-get update && apt-get install -y ffmpeg\n",
            "\n",
            "=== 실시간 한국어 음성인식 ===\n",
            "1. 모델 초기화 중...\n",
            "Wav2Vec2 한국어 모델 로딩 중...\n",
            "모델 로딩 완료! 사용 모델: w11wo/wav2vec2-xls-r-300m-korean\n",
            "2. 마이크 녹음 인터페이스 설정...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <script>\n",
              "    let mediaRecorder;\n",
              "    let audioChunks = [];\n",
              "    let isRecording = false;\n",
              "    \n",
              "    async function startRecording() {\n",
              "        try {\n",
              "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
              "            mediaRecorder = new MediaRecorder(stream);\n",
              "            audioChunks = [];\n",
              "            \n",
              "            mediaRecorder.ondataavailable = event => {\n",
              "                audioChunks.push(event.data);\n",
              "            };\n",
              "            \n",
              "            mediaRecorder.onstop = async () => {\n",
              "                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
              "                const arrayBuffer = await audioBlob.arrayBuffer();\n",
              "                const base64Audio = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));\n",
              "                \n",
              "                // Python으로 오디오 데이터 전송\n",
              "                google.colab.kernel.invokeFunction('process_audio', [base64Audio], {});\n",
              "            };\n",
              "            \n",
              "            mediaRecorder.start();\n",
              "            isRecording = true;\n",
              "            \n",
              "            document.getElementById('recordBtn').innerText = '녹음 중... (클릭하여 중지)';\n",
              "            document.getElementById('recordBtn').style.backgroundColor = '#ff4444';\n",
              "            \n",
              "        } catch (err) {\n",
              "            console.error('마이크 접근 오류:', err);\n",
              "            alert('마이크 접근 권한이 필요합니다.');\n",
              "        }\n",
              "    }\n",
              "    \n",
              "    function stopRecording() {\n",
              "        if (mediaRecorder && isRecording) {\n",
              "            mediaRecorder.stop();\n",
              "            isRecording = false;\n",
              "            \n",
              "            document.getElementById('recordBtn').innerText = '음성 인식 중...';\n",
              "            document.getElementById('recordBtn').disabled = true;\n",
              "            \n",
              "            // 스트림 중지\n",
              "            mediaRecorder.stream.getTracks().forEach(track => track.stop());\n",
              "        }\n",
              "    }\n",
              "    \n",
              "    function toggleRecording() {\n",
              "        if (isRecording) {\n",
              "            stopRecording();\n",
              "        } else {\n",
              "            startRecording();\n",
              "        }\n",
              "    }\n",
              "    \n",
              "    function resetButton() {\n",
              "        document.getElementById('recordBtn').innerText = ' 녹음 시작';\n",
              "        document.getElementById('recordBtn').style.backgroundColor = '#4CAF50';\n",
              "        document.getElementById('recordBtn').disabled = false;\n",
              "    }\n",
              "    </script>\n",
              "    \n",
              "    <div style=\"text-align: center; margin: 20px;\">\n",
              "        <button id=\"recordBtn\" onclick=\"toggleRecording()\" \n",
              "                style=\"padding: 15px 30px; font-size: 16px; background-color: #4CAF50; \n",
              "                       color: white; border: none; border-radius: 5px; cursor: pointer;\">\n",
              "             녹음 시작\n",
              "        </button>\n",
              "        <div id=\"result\" style=\"margin-top: 20px; padding: 10px; \n",
              "                              background-color: #f0f0f0; border-radius: 5px; \n",
              "                              min-height: 50px; font-size: 14px;\">\n",
              "            인식 결과가 여기에 표시됩니다...\n",
              "        </div>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. 설정 완료!\n",
            "위의 '🎤 녹음 시작' 버튼을 클릭하여 음성인식을 시작하세요.\n",
            "녹음을 중지하려면 버튼을 다시 클릭하세요.\n",
            "\n",
            " 모델 성능 정보:\n",
            "현재 사용 모델: w11wo/wav2vec2-xls-r-300m-korean - 가장 높은 정확도 (300M 파라미터)\n",
            "\n",
            "💡 다른 모델 사용법:\n",
            "start_realtime_stt('fast')      # 빠른 처리\n",
            "start_realtime_stt('balanced')  # 균형잡힌 성능\n",
            "start_realtime_stt('senior')    # 시니어 특화\n",
            "\n",
            " 추천 모델 성능 순위:\n",
            "1. w11wo/wav2vec2-xls-r-300m-korean (가장 높은 정확도)\n",
            "2. kresnik/wav2vec2-large-xlsr-korean (현재 기본 모델)\n",
            "3. Hyuk/wav2vec2-korean-v2 (개선된 v2 버전)\n",
            "4. hyyoka/wav2vec2-xlsr-korean-senior (시니어 음성 특화)\n",
            "\n",
            "💡 사용 예시:\n",
            "start_realtime_stt('best')      # 최고 성능 모델\n",
            "start_realtime_stt('fast')      # 빠른 처리 모델\n",
            "start_realtime_stt('senior')    # 시니어 특화 모델\n"
          ]
        }
      ]
    }
  ]
}