{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "실제 음성 변환 모델의 구현"
      ],
      "metadata": {
        "id": "BpsesfEbuQOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9kLGaNHserC"
      },
      "outputs": [],
      "source": [
        "# 1) 라이브러리 설치\n",
        "!pip install -q librosa soundfile\n",
        "\n",
        "# 2) 파일 업로드 (source.wav 라는 이름의 .wav 파일을 준비하세요)\n",
        "from google.colab import files\n",
        "print(\"▶ 5–10초 분량의 source.wav 파일을 업로드하세요\")\n",
        "uploaded = files.upload()\n",
        "source_file = next(f for f in uploaded if f.lower().endswith(\".wav\"))\n",
        "print(\"업로드된 파일:\", source_file)\n",
        "\n",
        "# 3) 음성 로드\n",
        "import librosa, soundfile as sf\n",
        "y, sr = librosa.load(source_file, sr=None)  # 원본 그대로의 샘플링레이트 유지\n",
        "\n",
        "# 4) 피치 쉬프트 (n_steps = +4 하면 반음 4개 올림, 음색이 높아짐)\n",
        "n_steps = 4  # 원하는 만큼 조절 (음색을 낮추려면 음수가능, e.g. -4)\n",
        "y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n",
        "\n",
        "# 5) (선택) 속도 변경도 해보고 싶다면 time_stretch 사용\n",
        "rate = 1.2  # 1보다 크면 빨라짐, 작으면 느려짐\n",
        "y_speed = librosa.effects.time_stretch(y_shifted, rate=rate)\n",
        "\n",
        "# 6) 결과 저장\n",
        "output_file = \"converted_voice.wav\"\n",
        "sf.write(output_file, y_shifted, sr)\n",
        "#sf.write(output_file, y_speed, sr) #속도 변경도 할 경우 위 코드는 주석처리 해야한다.\n",
        "print(\"저장된 변환 음성:\", output_file)\n",
        "\n",
        "# 7) Colab 상에서 재생\n",
        "from IPython.display import Audio, display\n",
        "display(Audio(output_file, rate=sr))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다양한 음성변환"
      ],
      "metadata": {
        "id": "PIHilrmR5_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 의존성 설치\n",
        "!pip install -q librosa soundfile numpy\n",
        "\n",
        "# 2) .wav 파일 업로드\n",
        "from google.colab import files\n",
        "print(\"5~10초 분량의 source.wav 파일을 업로드하세요\")\n",
        "uploaded = files.upload()\n",
        "src = next(f for f in uploaded.keys() if f.lower().endswith(\".wav\"))\n",
        "print(\"사용할 파일:\", src)\n",
        "\n",
        "# 3) 라이브러리 임포트\n",
        "import numpy as np\n",
        "import librosa, soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# 4) 원본 로드\n",
        "y, sr = librosa.load(src, sr=None)\n",
        "print(f\"Loaded {src}, samplerate = {sr}, duration = {y.shape[0]/sr:.2f}s\")\n",
        "\n",
        "# 5) 효과 함수 정의\n",
        "\n",
        "def pitch_shift(y, sr, n_steps):  #pitch 주파수 이동\n",
        "    return librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n",
        "\n",
        "def time_stretch(y, rate):  #sampling rate 변환 -> 속도 조절\n",
        "    return librosa.effects.time_stretch(y, rate=rate)\n",
        "\n",
        "def echo(y, sr, delay=0.3, attenuation=0.6): #반향 -> echo(매아리) 소리가 장애물에 부딪쳐서 돌아온는 소리다. 즉 원본 소리와 크기가 줄어든 소리가 계속 흐른다.\n",
        "    nd = int(delay * sr)\n",
        "    y_echo = np.zeros_like(y)\n",
        "    y_echo[nd:] = y[:-nd] * attenuation\n",
        "    return y + y_echo\n",
        "\n",
        "def reverb(y, sr, reverb_scale=0.5): #잔향 -> 여러 개의 path가 적용된다. 예를 들어 뒤에 스피커가 있으면 이 스피커에서 시작된 소리가 내 귀로 들어올 때까지의 경로는 여러 개이다. 각 경로마다 소리의 길이가 다르다. 넓은 공간에서 소리를 듣는 것처럼 느끼게 된다.\n",
        "    # 짧은 지수 감쇠 임펄스 응답 생성 (30ms)\n",
        "    ir_len = int(0.03 * sr)\n",
        "    ir = np.logspace(0, -3, ir_len)\n",
        "    y_rev = np.convolve(y, ir)[:len(y)]\n",
        "    return (1 - reverb_scale) * y + reverb_scale * y_rev\n",
        "\n",
        "def distortion(y, threshold=0.3): #원래 음을 깨지도록 함\n",
        "    return np.clip(y, -threshold, threshold)\n",
        "\n",
        "# 6) 각 이펙트 적용 & 저장 & 재생\n",
        "\n",
        "effects = {\n",
        "    \"pitch_up_4\":  pitch_shift(y, sr, +4),\n",
        "    \"pitch_down_4\":pitch_shift(y, sr, -4),\n",
        "    \"slow_0.8\":    time_stretch(y, 0.8),\n",
        "    \"fast_1.2\":    time_stretch(y, 1.2),\n",
        "    \"echo\":        echo(y, sr, delay=0.25, attenuation=0.5),\n",
        "    \"reverb\":      reverb(y, sr, reverb_scale=0.4),\n",
        "    \"distortion\":  distortion(y, threshold=0.2),\n",
        "}\n",
        "\n",
        "for name, y_eff in effects.items():\n",
        "    # 길이 맞추기 (time_stretch만 길이가 다름)\n",
        "    if len(y_eff) > len(y):\n",
        "        y_eff = y_eff[:len(y)]\n",
        "    else:\n",
        "        y_eff = np.pad(y_eff, (0, len(y) - len(y_eff)))\n",
        "    fn = f\"out_{name}.wav\"\n",
        "    sf.write(fn, y_eff, sr)\n",
        "    print(f\"▶ {name} saved to {fn}\")\n",
        "    display(name, Audio(fn, rate=sr))"
      ],
      "metadata": {
        "id": "mZUItmiI6A3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "voice cloning\n",
        "\n",
        "음성 복제 즉 목소리 흉내이다.\n",
        "\n"
      ],
      "metadata": {
        "id": "NSmFDNLC-1Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Voice Cloning\n",
        "\n",
        "# 1. 필요한 라이브러리 설치\n",
        "print(\"Installing necessary libraries...\")\n",
        "!pip install -q transformers accelerate scipy librosa soundfile numpy\n"
      ],
      "metadata": {
        "id": "AnX1qRSH_uSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. 모듈 임포트\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# 3. 디바이스 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 4. Bark 모델 로드 (데모용, 실제 사용 시 TARGET_BARK 는 이미 준비된 16kHz 파일)\n",
        "print(\"Loading Bark model and processor...\")\n",
        "processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n",
        "model = AutoModel.from_pretrained(\"suno/bark-small\").to(device)\n",
        "print(\"Bark model loaded successfully!\")\n",
        "\n",
        "# 5. Bark 합성 함수 정의 (필요시)\n",
        "def generate_bark_audio(text_input, speaker_preset=\"v2/en_speaker_6\", filename=\"bark_output.wav\"):\n",
        "    inputs = processor(text_input, voice_preset=speaker_preset)\n",
        "    for k, v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            inputs[k] = v.to(device)\n",
        "    with torch.no_grad():\n",
        "        speech_output = model.generate(**inputs, do_sample=True, fine_tuned_audio_output=True)\n",
        "    # Bark는 24kHz로 생성하므로, 이후 16kHz로 리샘플할 것\n",
        "    tmp_wav = \"tmp_bark.wav\"\n",
        "    write_wav(tmp_wav, 24000, speech_output.cpu().numpy().squeeze())\n",
        "    # 16kHz 리샘플\n",
        "    y, _ = librosa.load(tmp_wav, sr=16000)\n",
        "    write_wav(filename, 16000, (y * 32767).astype(np.int16))\n",
        "    os.remove(tmp_wav)\n",
        "    return filename\n",
        "\n",
        "# 6. Voice Cloning: 피치 매칭 방식 (샘플링레이트 16kHz)\n",
        "print(\"▶ Upload your reference voice AND your target TTS .wav files (2 files, both 16kHz).\")\n",
        "uploaded = files.upload()\n",
        "wav_files = [f for f in uploaded if f.lower().endswith(\".wav\")]\n",
        "if len(wav_files) != 2:\n",
        "    raise RuntimeError(\"두 개의 16kHz .wav 파일(참조 음성, TTS 결과)을 업로드해야 합니다.\")\n",
        "REF_VOICE, TARGET_BARK = wav_files #REF_VOICE에서 특징 뽑아서 TARGET으로 갔다.\n",
        "                                   #REF_VOICE의 Pitch만 뽑아서 TARGET으로 넘긴 것이다.\n",
        "                                   #이때 TARGET도 18일차에 녹음한 나의 목소리이지만\n",
        "                                   #REF_VOICE의 Pitch를 강제로 TARGET에다가 넣은 것이다.\n",
        "\n",
        "print(f\"Reference voice: {REF_VOICE}\")\n",
        "print(f\"Target TTS output: {TARGET_BARK}\")\n",
        "\n",
        "# 7. 파일 로드 및 피치 매칭 (sr=16000)\n",
        "y_ref, sr_ref = librosa.load(REF_VOICE, sr=16000)\n",
        "y_tgt, sr_tgt = librosa.load(TARGET_BARK, sr=16000)\n",
        "\n",
        "# 피치(F0) 추정\n",
        "f0_ref = librosa.yin(y_ref, fmin=50, fmax=500, sr=sr_ref)\n",
        "f0_tgt = librosa.yin(y_tgt, fmin=50, fmax=500, sr=sr_tgt)\n",
        "avg_ref = np.nanmean(f0_ref)\n",
        "avg_tgt = np.nanmean(f0_tgt)\n",
        "n_steps = 12 * np.log2(avg_ref / avg_tgt)\n",
        "\n",
        "# 피치 시프트 (모두 키워드 인자로 전달)\n",
        "y_shifted = librosa.effects.pitch_shift(\n",
        "    y=y_tgt,\n",
        "    sr=sr_tgt,\n",
        "    n_steps=n_steps\n",
        ")\n",
        "\n",
        "# 8. 결과 저장 및 재생 (16kHz)\n",
        "CLONED_OUT = \"bark_cloned_16k.wav\"\n",
        "write_wav(CLONED_OUT, 16000, (y_shifted * 32767).astype(np.int16))\n",
        "print(f\"✅ Cloned voice saved to {CLONED_OUT}\")\n",
        "ipd.display(ipd.Audio(CLONED_OUT, rate=16000))"
      ],
      "metadata": {
        "id": "bMOLWjuR-3jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Voice Cloning (Pitch + Speed Clone, 16 kHz)\n",
        "\n",
        "# 1. 필요한 라이브러리 설치\n",
        "!pip install -q transformers accelerate scipy librosa soundfile numpy\n",
        "\n",
        "# 2. 모듈 임포트\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# 3. 디바이스 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 4. Bark 모델 로드 (데모용)\n",
        "processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n",
        "model = AutoModel.from_pretrained(\"suno/bark-small\").to(device)\n",
        "\n",
        "# 5. Bark 합성 함수 (필요 시)\n",
        "def generate_bark_audio(text_input, speaker_preset=\"v2/en_speaker_6\", filename=\"bark_output.wav\"):\n",
        "    inputs = processor(text_input, voice_preset=speaker_preset)\n",
        "    for k, v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            inputs[k] = v.to(device)\n",
        "    with torch.no_grad():\n",
        "        speech_output = model.generate(**inputs, do_sample=True, fine_tuned_audio_output=True)\n",
        "    tmp = \"tmp_bark.wav\"\n",
        "    write_wav(tmp, 24000, speech_output.cpu().numpy().squeeze())\n",
        "    y, _ = librosa.load(tmp, sr=16000)\n",
        "    write_wav(filename, 16000, (y * 32767).astype(np.int16))\n",
        "    os.remove(tmp)\n",
        "    return filename\n",
        "\n",
        "# 6. Voice Cloning: 피치와 속도 클론\n",
        "print(\"Upload two 16kHz WAV files: reference voice and target TTS output.\")\n",
        "uploaded = files.upload()\n",
        "wav_files = [f for f in uploaded if f.lower().endswith(\".wav\")]\n",
        "if len(wav_files) != 2:\n",
        "    raise RuntimeError(\"두 개의 16kHz WAV 파일을 업로드해야 합니다.\")\n",
        "REF_VOICE, TARGET_BARK = wav_files\n",
        "\n",
        "# 7. 파일 로드\n",
        "sr = 16000\n",
        "y_ref, _ = librosa.load(REF_VOICE, sr=sr) #화자의 특징을 뽑기 위함\n",
        "y_tgt, _ = librosa.load(TARGET_BARK, sr=sr) #화자의 특징을 적용하기 위함\n",
        "\n",
        "# 8. 속도 매칭\n",
        "dur_ref = len(y_ref) / sr\n",
        "dur_tgt = len(y_tgt) / sr\n",
        "rate = dur_tgt / dur_ref\n",
        "y_rate = librosa.effects.time_stretch(y=y_tgt, rate=rate)\n",
        "\n",
        "# 9. 피치 매칭\n",
        "f0_ref = librosa.yin(y=y_ref, fmin=50, fmax=500, sr=sr)\n",
        "f0_rate = librosa.yin(y=y_rate, fmin=50, fmax=500, sr=sr)\n",
        "n_steps = 12 * np.log2(np.nanmean(f0_ref) / np.nanmean(f0_rate))\n",
        "y_shift = librosa.effects.pitch_shift(y=y_rate, sr=sr, n_steps=n_steps)\n",
        "\n",
        "# 10. 길이 맞추기\n",
        "if len(y_shift) > len(y_ref):\n",
        "    y_final = y_shift[:len(y_ref)]\n",
        "else:\n",
        "    y_final = np.pad(y_shift, (0, len(y_ref) - len(y_shift)))\n",
        "\n",
        "# 11. 저장 및 재생\n",
        "OUT = \"voice_cloned_full.wav\"\n",
        "write_wav(OUT, sr, (y_final * 32767).astype(np.int16))\n",
        "print(\"Cloned voice saved to\", OUT)\n",
        "ipd.display(ipd.Audio(REF_VOICE, rate=sr))\n",
        "ipd.display(ipd.Audio(OUT, rate=sr))"
      ],
      "metadata": {
        "id": "RDDb5FwMH5bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Voice Cloning (Reference + Hardcoded Text, 16 kHz)\n",
        "\n",
        "# 1. 필요한 라이브러리 설치\n",
        "!pip install -q transformers accelerate scipy librosa soundfile numpy\n",
        "\n",
        "# 2. 모듈 임포트\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# 3. 디바이스 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 4. Bark 모델 로드 (24 kHz→16 kHz 용)\n",
        "processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n",
        "model     = AutoModel.from_pretrained(\"suno/bark-small\").to(device)\n",
        "\n",
        "# 5. Hardcoded 텍스트\n",
        "SYNTH_TEXT = \"안녕하세요. 이 목소리로 새로운 문장을 합성합니다.\"\n",
        "\n",
        "# 6. Bark TTS 생성 함수 (24→16 kHz)\n",
        "def synthesize_bark(text, out16k=\"tts_16k.wav\"):\n",
        "    inputs = processor(text, voice_preset=\"v2/ko_speaker_6\")\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            inputs[k] = v.to(device)\n",
        "    with torch.no_grad():\n",
        "        speech = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            fine_tuned_audio_output=True\n",
        "        )\n",
        "    # 24 kHz 저장 → 16 kHz 리샘플\n",
        "    tmp = \"tmp24k.wav\"\n",
        "    write_wav(tmp, 24000, speech.cpu().numpy().squeeze())\n",
        "    y, _ = librosa.load(tmp, sr=16000)\n",
        "    write_wav(out16k, 16000, (y * 32767).astype(np.int16))\n",
        "    os.remove(tmp)\n",
        "    return out16k\n",
        "\n",
        "# 7. 참조 음성 업로드 (16 kHz WAV 한 개)\n",
        "print(\"▶ Upload ONE reference voice (16 kHz WAV):\")\n",
        "uploaded = files.upload()\n",
        "wav_files = [f for f in uploaded if f.lower().endswith(\".wav\")]\n",
        "if len(wav_files) != 1:\n",
        "    raise RuntimeError(\"16 kHz WAV 파일을 한 개만 업로드해주세요.\")\n",
        "REF_VOICE = wav_files[0]\n",
        "print(\"Reference voice:\", REF_VOICE)\n",
        "\n",
        "# 8. Bark TTS 생성\n",
        "print(\"\\n▶ Generating TTS from hardcoded text …\")\n",
        "TTS_WAV = synthesize_bark(SYNTH_TEXT)\n",
        "print(\"Generated TTS file:\", TTS_WAV)\n",
        "ipd.display(ipd.Audio(TTS_WAV, rate=16000))\n",
        "\n",
        "# 9. 파일 로드 (sr=16 kHz)\n",
        "sr = 16000\n",
        "y_ref, _ = librosa.load(REF_VOICE, sr=sr)\n",
        "y_tgt, _ = librosa.load(TTS_WAV,  sr=sr)\n",
        "\n",
        "# 10. 속도 매칭\n",
        "dur_ref = len(y_ref) / sr\n",
        "dur_tgt = len(y_tgt) / sr\n",
        "rate    = dur_tgt / dur_ref\n",
        "y_rate  = librosa.effects.time_stretch(y=y_tgt, rate=rate)\n",
        "\n",
        "# 11. 피치 매칭\n",
        "f0_ref  = librosa.yin(y=y_ref,  fmin=50, fmax=500, sr=sr)\n",
        "f0_rt   = librosa.yin(y=y_rate, fmin=50, fmax=500, sr=sr)\n",
        "n_steps = 12 * np.log2(np.nanmean(f0_ref) / np.nanmean(f0_rt))\n",
        "y_clone = librosa.effects.pitch_shift(y=y_rate, sr=sr, n_steps=n_steps)\n",
        "\n",
        "# 12. 길이 맞추기 (참조 음성 기준)\n",
        "if len(y_clone) > len(y_ref):\n",
        "    y_final = y_clone[:len(y_ref)]\n",
        "else:\n",
        "    y_final = np.pad(y_clone, (0, len(y_ref) - len(y_clone)))\n",
        "\n",
        "# 13. 저장 및 재생\n",
        "OUT = \"voice_cloned_full.wav\"\n",
        "write_wav(OUT, sr, (y_final * 32767).astype(np.int16))\n",
        "print(\"\\n Cloned voice saved:\", OUT)\n",
        "print(\"▶ Reference voice:\")\n",
        "ipd.display(ipd.Audio(REF_VOICE, rate=sr))\n",
        "print(\"▶ Cloned output:\")\n",
        "ipd.display(ipd.Audio(OUT, rate=sr))"
      ],
      "metadata": {
        "id": "3Tzm6dsYMZHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bark실습"
      ],
      "metadata": {
        "id": "AeX9n6c9yBT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Bark TTS 모델 설치 및 데모 실행 (단일 스크립트)\n",
        "\n",
        "# 1. 필요한 라이브러리 설치\n",
        "# transformers: Bark 모델 로드 및 사용\n",
        "# accelerate: 모델 로딩 및 실행 속도 최적화 (GPU 사용 시)\n",
        "# scipy: 오디오 파일 저장\n",
        "\n",
        "#이 코드를 실행하기 위해서는 cpu모드로 전환해야 한다.\n",
        "print(\"Installing necessary libraries...\")\n",
        "!pip install -q transformers accelerate scipy\n",
        "\n",
        "# 2. 필요한 모듈 임포트\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "import IPython.display as ipd # Colab에서 오디오 재생을 위해\n",
        "\n",
        "# 3. GPU 사용 가능 여부 확인 및 디바이스 설정\n",
        "# Colab에서 '런타임' -> '런타임 유형 변경' -> 'T4 GPU' 선택 권장\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"CUDA (GPU) is available! Using GPU for Bark.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"CUDA (GPU) is not available. Using CPU. This may be slow.\")\n",
        "\n",
        "# 4. Bark 모델 및 프로세서 로드\n",
        "# suno/bark-small은 더 가벼운 버전이며, suno/bark는 풀 사이즈 모델입니다.\n",
        "# 처음 실행 시 모델을 다운로드하므로 시간이 걸릴 수 있습니다.\n",
        "print(\"Loading Bark model and processor...\")\n",
        "processor = AutoProcessor.from_pretrained(\"suno/bark-small\") # 또는 \"suno/bark\"\n",
        "model = AutoModel.from_pretrained(\"suno/bark-small\").to(device) # 또는 \"suno/bark\"\n",
        "print(\"Bark model loaded successfully!\")\n",
        "\n",
        "# 5. 텍스트를 음성으로 변환하는 함수 정의\n",
        "def generate_bark_audio(text_input, speaker_preset=\"v2/en_speaker_6\", filename=\"bark_output.wav\"):\n",
        "    \"\"\"\n",
        "    Bark 모델을 사용하여 텍스트 프롬프트를 오디오로 변환합니다.\n",
        "\n",
        "    Args:\n",
        "        text_input (str): 음성으로 변환할 텍스트.\n",
        "                           감성이나 어조를 표현하는 텍스트도 포함할 수 있습니다.\n",
        "                           예: \"안녕하세요. [행복한 표정] 오늘 날씨가 정말 좋네요!\"\n",
        "        speaker_preset (str): 사용할 스피커 프리셋 (예: \"v2/en_speaker_6\", \"v2/ko_speaker_6\").\n",
        "                              다양한 프리셋이 있으며, 각각 다른 목소리와 억양을 가집니다.\n",
        "                              한국어는 'v2/ko_speaker_X' 형식으로 시도할 수 있습니다.\n",
        "        filename (str): 생성된 오디오 파일을 저장할 이름.\n",
        "    \"\"\"\n",
        "    print(f\"\\nGenerating audio for text: '{text_input}'\")\n",
        "    print(f\"Using speaker preset: '{speaker_preset}'\")\n",
        "\n",
        "    # 입력 텍스트를 Bark 모델에 맞는 형태로 인코딩\n",
        "    inputs = processor(text_input, voice_preset=speaker_preset).to(device)\n",
        "\n",
        "    # 오디오 생성\n",
        "    with torch.no_grad():\n",
        "        speech_output = model.generate(**inputs, do_sample=True, fine_tuned_audio_output=True)\n",
        "\n",
        "    # 생성된 오디오를 WAV 파일로 저장\n",
        "    # Bark 모델의 기본 샘플링 레이트는 24000 Hz입니다.\n",
        "    bark_sample_rate = 24000\n",
        "    write_wav(filename, bark_sample_rate, speech_output.cpu().numpy().squeeze())\n",
        "    print(f\"Audio saved to '{filename}' with sample rate {bark_sample_rate} Hz.\")\n",
        "\n",
        "    # Colab에서 오디오 재생\n",
        "    print(\"Playing audio...\")\n",
        "    return ipd.Audio(filename)\n",
        "\n",
        "# 6. Bark TTS 데모 실행 예시\n",
        "\n",
        "# 예시 1: 영어 (기본 스피커 프리셋)\n",
        "english_text = \"Hello everyone. [laughs] I hope you are having a wonderful day today!\"\n",
        "generate_bark_audio(english_text, filename=\"english_demo.wav\")\n",
        "\n",
        "# 예시 2: 한국어\n",
        "# 한국어 스피커 프리셋은 'v2/ko_speaker_X' 형식으로 시도할 수 있습니다.\n",
        "# X는 0부터 9까지의 숫자일 수 있으며, 각각 다른 목소리를 가집니다.\n",
        "korean_text_1 = \"안녕하세요. 오늘 날씨가 정말 좋네요. 즐거운 하루 보내세요!\"\n",
        "generate_bark_audio(korean_text_1, speaker_preset=\"v2/ko_speaker_6\", filename=\"korean_demo_1.wav\")\n",
        "\n",
        "korean_text_2 = \"중요한 공지입니다. 모든 직원분들은 회의실로 모여주시기 바랍니다. [진지한 어조]\"\n",
        "generate_bark_audio(korean_text_2, speaker_preset=\"v2/ko_speaker_9\", filename=\"korean_demo_2.wav\")\n",
        "\n",
        "korean_text_3 = \"정말 믿을 수가 없어! [놀란 어조] 이게 사실이라고?\"\n",
        "generate_bark_audio(korean_text_3, speaker_preset=\"v2/ko_speaker_0\", filename=\"korean_demo_3.wav\")\n",
        "\n",
        "# 참고: Bark는 텍스트 프롬프트 안에 [laughs], [sighs], [music], [narrator] 등\n",
        "# 다양한 지시어를 포함하여 음향 효과나 어조를 유도할 수 있습니다.\n",
        "# 하지만 모든 지시어가 항상 완벽하게 작동하는 것은 아니며, 한국어의 경우 더욱 실험이 필요할 수 있습니다."
      ],
      "metadata": {
        "id": "RfvxNYt5x-z2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}